# FLOWNO Notebook — README

This README documents the `FLOWNO.ipynb` notebook included in this repository. The notebook demonstrates preprocessing of the Google Speech Commands dataset, building a simple CNN, and exporting a quantized TFLite model for edge inference.

## Quick summary
- Notebook file: `FLOWNO.ipynb`
- Purpose: Load a small subset of the Google Speech Commands dataset, convert `.wav` audio into Mel-spectrograms, train/simulate a CNN, convert the model to TFLite INT8, and show example inference with the TFLite interpreter.
- Output artifact: `qcnn_model.tflite` (quantized model saved by the notebook).

## Requirements
- Python 3.8+
- Recommended packages (see `requirements.txt`): `tensorflow`, `librosa`, `numpy`, `matplotlib`, `scikit-learn`

Install dependencies:

```bash
pip install -r requirements.txt
```

Or (if `requirements.txt` is not available):

```bash
pip install tensorflow librosa numpy matplotlib scikit-learn
```

## How to run
- Locally (recommended in a virtual environment):
  1. Ensure dependencies are installed.
  2. Open `FLOWNO.ipynb` in JupyterLab / Jupyter Notebook and run cells sequentially.

- In Google Colab:
  1. Upload `FLOWNO.ipynb` to Colab or open from GitHub.
  2. Run the cells; the notebook contains a cell that downloads the Speech Commands tarball.

Notes:
- The notebook uses `wget` and `tar` to download/extract `speech_commands_v0.02.tar.gz` (cells 2–3). In Colab these commands work; on pure Windows you may need WSL or download/extract manually.

## Notebook cell-by-cell overview
(Refer to cell numbers when viewing the notebook; do not rely on internal cell IDs.)

- Cell 1: Import core libraries (`numpy`, `matplotlib`) and audio/ML libraries (`librosa`, `tensorflow`).
- Cell 2: Download the Speech Commands dataset archive via `wget` and extract it (calls `tar`).
- Cell 3: (Duplicate extraction) additional `tar` extraction command present.
- Cell 4: Collect file paths and labels for the target classes (`on`, `happy`, `follow`), print counts, and visualize one waveform using `librosa.display`.
- Cell 5: Define `audio_to_melspec()` to convert audio files into Mel-spectrograms; test and visualize a spectrogram.
- Cell 6: Build arrays `X` and `y` by processing `.wav` files per class, expanding dims for channels, and printing shapes.
- Cell 7: Convert labels to categorical and split into train/test sets with `train_test_split`.
- Cell 8: Define a small CNN model using Keras `Sequential` (Conv2D/MaxPool layers → Dense → softmax) and compile it.
- Cell 9: Train the model using `model.fit()` (example training loop with `epochs=20`, `batch_size=32`).
- Cell 10: Convert the trained Keras model to a fully integer-quantized TFLite model using a representative dataset generator; save as `qcnn_model.tflite`.
- Cell 11: Example TFLite interpreter usage: load `qcnn_model.tflite`, quantize a test input using interpreter quantization params, invoke the model, and dequantize outputs for inspection.
- Cell 12: Create a `saved_models` folder and move `qcnn_model.tflite` into it.

## Notes & tips
- Audio length & sampling: The notebook pads/trims each audio to `duration = 1.0s` at `sr = 16000` Hz. Adjust `duration` if your dataset contains longer examples.
- Representative dataset: For robust integer quantization, ensure the `representative_data_gen()` yields realistic float32 inputs matching preprocessing and normalization.
- Windows users: `wget` and POSIX `tar` commands may not be available by default. Download the dataset via a browser or use WSL/PowerShell alternatives.
- Memory: Converting all audio to spectrograms and forming `X` may use significant RAM. Consider a generator-based pipeline or smaller batches if you hit memory limits.

## Expected artifacts
- `qcnn_model.tflite` — quantized TFLite model generated by the notebook.

## License
This notebook and the generated README follow the repository's MIT License.

---

If you want, I can:
- Add a short runnable script that runs the same steps outside the notebook (Python module), or
- Create a minimal `requirements.txt` (if missing) and test the notebook execution in Colab-friendly mode.

Which would you prefer next?